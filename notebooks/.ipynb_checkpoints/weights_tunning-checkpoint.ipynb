{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optiver_trading_at_the_close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptiver_trading_at_the_close\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_engineering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FE\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptiver_trading_at_the_close\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnearest_neightbors_features\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighborsFeatures\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptiver_trading_at_the_close\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolumn_selector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnSelector\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optiver_trading_at_the_close'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from optiver_trading_at_the_close.feature_engineering import FE\n",
    "from optiver_trading_at_the_close.nearest_neightbors_features import NearestNeighborsFeatures\n",
    "from optiver_trading_at_the_close.column_selector import ColumnSelector\n",
    "from optiver_trading_at_the_close.memory_reduction import MemoryReduction\n",
    "from optiver_trading_at_the_close.mean_regressor_ensemble import MeanRegressorEnsemble\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './../data/train.csv'\n",
    "\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    # './../output/models/V3/lightgbm/lightgbm-89ab1659-547a-4845-94b8-793690fbcee0.joblib',\n",
    "    # './../output/models/V3/lightgbm/lightgbm-910699bf-8877-4b40-b01a-2e887d23616d.joblib',\n",
    "    # './../output/models/V3/lightgbm/lightgbm-b931c388-7be7-4fd7-a1ad-6002a444fe2c.joblib',\n",
    "    './../output/models/V3/lightgbm/lightgbm-ec04e2b2-6681-4436-b694-ebbd62880006.joblib',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=['target'], axis=0)\n",
    "\n",
    "# y = df['target']\n",
    "# X = df.drop(columns='target')\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['target'], axis=0)\n",
    "\n",
    "X_train = df.loc[df['date_id'] <= 420]\n",
    "X_test = df.loc[df['date_id'] > 420]\n",
    "\n",
    "y_train = X_train['target']\n",
    "# X_train = X_train.drop(columns='target')\n",
    "\n",
    "y_test = X_test['target']\n",
    "# X_test = X_test.drop(columns='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [joblib.load(model_path) for model_path in MODEL_PATHS]\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('fe', FE()),\n",
    "    # ('nn_features', NearestNeighborsFeatures(\n",
    "    #     features_to_use_for_distance_computation=['seconds_in_bucket', 'wap', 'bid_plus_ask_sizes', 'bid_ask_size_imb'],\n",
    "    #     get_target=True,\n",
    "    #     features_get=['wap', 'bid_ask_size_imb'],\n",
    "    #     n_neighbors=[40],\n",
    "    #     metrics=['l1'],\n",
    "    #     n_jobs=-1\n",
    "    # )),\n",
    "    ('column_selector', ColumnSelector(cols_to_drop=['time_id', 'row_id', 'date_id', 'target'])),\n",
    "    ('memore_reduction', MemoryReduction()),\n",
    "    ('mean_regressor_ensemble', MeanRegressorEnsemble(estimators))\n",
    "])\n",
    "\n",
    "# pipeline.fit(X_train, y_train, mean_regressor_ensemble__fit_estimators=False)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get each model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_each_model_predictions(pipeline, X):\n",
    "    _X = X.copy()\n",
    "\n",
    "    for step in pipeline.steps[:-1]:\n",
    "        _X = step[1].transform(_X)\n",
    "        \n",
    "    preds = [estimator.predict(_X, nn_features__exclude_self=False) for estimator in pipeline.steps[-1][1].estimators]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = get_each_model_predictions(pipeline, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_weights(n, grid=np.arange(0, 1.01, 0.01)):\n",
    "    grid = grid\n",
    "    results = []\n",
    "    \n",
    "    def find_combinations(target_sum, current_combination, start_index):\n",
    "        if target_sum == 0 and len(current_combination) == n:\n",
    "            results.append(current_combination)\n",
    "            return\n",
    "        if target_sum < 0 or len(current_combination) == n:\n",
    "            return\n",
    "        for i in range(start_index, len(grid)):\n",
    "            new_combination = current_combination + [grid[i]]\n",
    "            find_combinations(target_sum - grid[i], new_combination, i)\n",
    "\n",
    "    find_combinations(1, [], 0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "weights = get_all_possible_weights(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_sum(listOfPrices, listOfVolumes):\n",
    "    \"\"\"\n",
    "    Source - https://www.kaggle.com/code/kaito510/goto-conversion-optiver-baseline-models\n",
    "    \"\"\"\n",
    "    \n",
    "    #compute standard errors assuming standard deviation is same for all stocks\n",
    "    listOfSe = np.sqrt(listOfVolumes)\n",
    "    step = sum(listOfPrices)/sum(listOfSe)\n",
    "    outputListOfPrices = listOfPrices - listOfSe*step\n",
    "    return outputListOfPrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min = np.min(y_train)\n",
    "y_max = np.max(y_train)\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for weight in tqdm(weights):\n",
    "    pred = np.average(preds, axis=0, weights=weight)\n",
    "    pred = zero_sum(pred, X_test.loc[:,'bid_size'] + X_test.loc[:,'ask_size'])\n",
    "    pred = np.clip(pred, y_min, y_max)\n",
    "    \n",
    "    results[tuple(weight)] = mean_absolute_error(y_test, pred)\n",
    "    \n",
    "results = sorted(results.items(), key=lambda x:x[1])\n",
    "results # 5.692617867330589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pipeline.predict(X_train)\n",
    "\n",
    "# train_pred = zero_sum(train_pred, X_train.loc[:,'bid_size'] + X_train.loc[:,'ask_size'])\n",
    "# train_pred = np.clip(train_pred, y_min, y_max)\n",
    "\n",
    "test_pred = pipeline.predict(X_test)\n",
    "\n",
    "# test_pred = zero_sum(test_pred, X_test.loc[:,'bid_size'] + X_test.loc[:,'ask_size'])\n",
    "# test_pred = np.clip(test_pred, y_min, y_max)\n",
    "\n",
    "print(f'Train = {mean_absolute_error(y_train, train_pred):.3f}')\n",
    "print(f'Test = {mean_absolute_error(y_test, test_pred):.3f}')\n",
    "\n",
    "# Train = 6.092\n",
    "# Test = 5.692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def get_booster_importance(lightgbm):\n",
    "    importance_types = ['gain', 'split']\n",
    "    \n",
    "    booster = lightgbm.booster_\n",
    "    \n",
    "    results = pd.DataFrame(index=booster.feature_name())\n",
    "    \n",
    "    for importance_type in importance_types:\n",
    "        importance_type_order = pd.DataFrame(\n",
    "            {importance_type:booster.feature_importance(importance_type=importance_type)},\n",
    "            index = booster.feature_name()\n",
    "        )\n",
    "\n",
    "        importance_type_order = importance_type_order.sort_values(by=importance_type, ascending=False)\n",
    "        importance_type_order[f'{importance_type}_importance'] = np.arange(1, importance_type_order.shape[0]+1)\n",
    "        importance_type_order.loc[importance_type_order[importance_type]==0, f'{importance_type}_importance'] = importance_type_order.shape[0]+1\n",
    "        \n",
    "        results.loc[results.index, f'{importance_type}_importance'] = importance_type_order.loc[results.index, f'{importance_type}_importance']\n",
    "        \n",
    "    results['average_importance'] = results.mean(axis=1)\n",
    "    \n",
    "    return results.sort_values(by='average_importance', ascending=True)\n",
    "\n",
    "aa = get_booster_importance(pipeline['mean_regressor_ensemble'].estimators[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 37))\n",
    "\n",
    "sns.barplot(\n",
    "    aa['average_importance'],\n",
    "    orient=\"y\",\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "lgb.plot_importance(\n",
    "    pipeline['mean_regressor_ensemble'].estimators[0],\n",
    "    importance_type=\"gain\",\n",
    "    figsize=(7,37),\n",
    "    title=\"LightGBM Feature Importance (Gain)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "lgb.plot_importance(\n",
    "    pipeline['mean_regressor_ensemble'].estimators[0],\n",
    "    importance_type=\"split\",\n",
    "    figsize=(7,37),\n",
    "    title=\"LightGBM Feature Importance (Gain)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import joblib\n",
    "# import uuid\n",
    "\n",
    "# DATA_PATH = './../data/train.csv'\n",
    "\n",
    "# VERSION = 'V4'\n",
    "\n",
    "# MODEL = 'lightgbm'\n",
    "\n",
    "# SAVE_MODEL_PATH_FOLDER = f'./../output/models/{VERSION}/lightgbm/'\n",
    "# SAVE_MODEL_BASE_NAME = 'lightgbm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for estimator in pipeline['mean_regressor_ensemble'].estimators:\n",
    "#     os.makedirs(SAVE_MODEL_PATH_FOLDER, exist_ok=True)\n",
    "#     joblib.dump(estimator, os.path.join(SAVE_MODEL_PATH_FOLDER, f'{SAVE_MODEL_BASE_NAME}-{str(uuid.uuid4())}.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore')\n",
    "\n",
    "# expected_preds = pd.DataFrame()\n",
    "\n",
    "# expected_preds[['date_id', 'seconds_in_bucket']] = df.loc[df['date_id'] >= 475, ['date_id', 'seconds_in_bucket']]\n",
    "# expected_preds['expected_preds'] = pipeline.predict(df.loc[df['date_id'] >= 475])\n",
    "# expected_preds = expected_preds.loc[expected_preds['date_id'] >= 478]\n",
    "# expected_preds.loc[:, 'submited_preds'] = np.array(preds).flatten()\n",
    "# expected_preds.loc[:, 'submited_preds_goto'] = pd.concat(preds_goto)['target'].values\n",
    "# expected_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.testing import assert_frame_equal\n",
    "\n",
    "# df_origin = df.tail(11000).reset_index(drop=True)\n",
    "# pred_current_df = current_day_df.reset_index(drop=True)\n",
    "\n",
    "# for step in pipeline.steps[:-1]:\n",
    "#     df_origin = step[1].transform(df_origin)\n",
    "#     pred_current_df = step[1].transform(pred_current_df)\n",
    "\n",
    "# assert_frame_equal(\n",
    "#     df_origin,\n",
    "#     pred_current_df,\n",
    "#     check_dtype=True,\n",
    "#     check_exact=True\n",
    "# #     atol=1e-6\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
